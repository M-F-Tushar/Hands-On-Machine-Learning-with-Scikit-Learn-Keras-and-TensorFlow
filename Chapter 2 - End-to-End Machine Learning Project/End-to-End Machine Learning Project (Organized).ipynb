{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d1ed928",
   "metadata": {},
   "source": [
    "# CHAPTER 2: End-to-End Machine Learning Project\n",
    "\n",
    "In this notebook, we will work through an end-to-end machine learning project using the California Housing Prices dataset. The goal is to illustrate the main steps of a machine learning project, from framing the problem to deploying a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae3002f",
   "metadata": {},
   "source": [
    "# CHAPTER 2: End-to-End Machine Learning Project\n",
    " \n",
    "Welcome to the Machine Learning Housing Corporation! In this notebook, you will work through an end-to-end machine learning project, pretending to be a newly hired data scientist at a real estate company. The goal is to illustrate the main steps of a machine learning project, not to learn anything about the real estate business itself.\n",
    "\n",
    "**Educational and Experimental Approach:**\n",
    "- This notebook follows the sequential structure of the book, with explanations and code for each step.\n",
    "- You are encouraged to experiment with the code, try different parameters, and observe the effects.\n",
    "- Explanations in markdown cells will help you understand the reasoning behind each step, common pitfalls, and best practices.\n",
    "\n",
    "**Project Steps:**\n",
    "1. Look at the big picture and frame the problem\n",
    "2. Get the data\n",
    "3. Explore and visualize the data to gain insights\n",
    "4. Prepare the data for machine learning algorithms\n",
    "5. Select a model and train it\n",
    "6. Fine-tune your model\n",
    "7. Present your solution\n",
    "8. Launch, monitor, and maintain your system\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d0fe3e",
   "metadata": {},
   "source": [
    "## Project Steps\n",
    "1. Look at the big picture\n",
    "2. Get the data\n",
    "3. Explore and visualize the data to gain insights\n",
    "4. Prepare the data for machine learning algorithms\n",
    "5. Select a model and train it\n",
    "6. Fine-tune your model\n",
    "7. Present your solution\n",
    "8. Launch, monitor, and maintain your system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c7fd00",
   "metadata": {},
   "source": [
    "## 1. Look at the Big Picture\n",
    "We are tasked with predicting the median housing price in California districts using census data. This is a supervised regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3e550b",
   "metadata": {},
   "source": [
    "## 1. Look at the Big Picture: Framing the Problem\n",
    "\n",
    "Before jumping into code, let's clarify the business objective and frame the machine learning problem.\n",
    "\n",
    "**Business Objective:**\n",
    "- Predict the median housing price in any California district, given census data.\n",
    "- The output will be used by a downstream system to help decide whether to invest in a given area.\n",
    "\n",
    "**Key Questions:**\n",
    "- What is the type of ML task? (Supervised, regression, batch learning)\n",
    "- What performance metric should we use? (RMSE, MAE)\n",
    "\n",
    "Let's define the RMSE and MAE formulas and discuss their differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bf2b4b",
   "metadata": {},
   "source": [
    "## 2. Get the Data\n",
    "The dataset is available as `housing.csv`. Let's load it and take a quick look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b998d763",
   "metadata": {},
   "source": [
    "## 2. Get the Data: Working with Real Data\n",
    "\n",
    "When learning machine learning, it's best to experiment with real-world data. There are many open datasets available online. In this project, we'll use the California Housing Prices dataset from the StatLib repository, based on the 1990 California census.\n",
    "\n",
    "**Why this dataset?**\n",
    "- It contains real census data, including population, median income, and housing prices for each district.\n",
    "- It's small enough to fit in memory, but large enough to be interesting.\n",
    "\n",
    "Let's load the data. If you don't have the CSV file, you can use the following function to download and extract it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58b733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load the California housing data (if not already present)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def load_housing_data():\n",
    "    tarball_path = Path(\"datasets/housing.tgz\")\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "    with tarfile.open(tarball_path) as housing_tarball:\n",
    "        housing_tarball.extractall(path=\"datasets\")\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b84916",
   "metadata": {},
   "source": [
    "### Inspecting the Data Structure\n",
    "\n",
    "Let's take a quick look at the data to understand its structure and contents. We'll use several pandas methods to get a feel for the dataset:\n",
    "- `.info()` for a summary of columns, types, and missing values\n",
    "- `.describe()` for statistics on numerical columns\n",
    "- `.value_counts()` for categorical columns\n",
    "- `.hist()` for visualizing distributions\n",
    "\n",
    "This is a crucial step before any modeling, as it helps you spot issues and understand what you're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b39834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a concise summary of the DataFrame, including missing values\n",
    "housing.info()  # Useful for understanding the data types and missing values\n",
    "\n",
    "# Check the distribution of the categorical attribute 'ocean_proximity'\n",
    "housing['ocean_proximity'].value_counts()  # Shows the number of districts in each category\n",
    "\n",
    "# Get summary statistics for numerical attributes\n",
    "housing.describe()  # Includes count, mean, std, min, max, and percentiles\n",
    "\n",
    "# Visualize the distribution of each numerical attribute\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "housing.hist(bins=50, figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1dd5bc",
   "metadata": {},
   "source": [
    "#### What did we learn from the data inspection?\n",
    "\n",
    "- The dataset has 20,640 instances and 10 attributes.\n",
    "- Most attributes are numerical, except for 'ocean_proximity', which is categorical.\n",
    "- The 'total_bedrooms' attribute has missing values (about 207 missing out of 20,640).\n",
    "- The 'ocean_proximity' column has a few distinct categories.\n",
    "- Some attributes (like 'median_income' and 'median_house_value') are capped or have unusual distributions.\n",
    "- Many attributes are right-skewed, which may affect some algorithms.\n",
    "\n",
    "**These insights will guide our data cleaning, feature engineering, and modeling choices.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8696956a",
   "metadata": {},
   "source": [
    "## 3. Create a Test Set\n",
    "\n",
    "Before exploring the data further, it's critical to set aside a test set. This helps prevent data snooping bias and ensures that our final evaluation is fair and unbiased.\n",
    "\n",
    "### Why create a test set now?\n",
    "- If you look at the test set during EDA or model selection, you risk overfitting to it.\n",
    "- The test set should only be used for the final evaluation, after all modeling decisions are made.\n",
    "\n",
    "Let's see several ways to split the data, including random splitting and stratified sampling based on income category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39130f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Random splitting (not stable across runs) ---\n",
    "import numpy as np\n",
    "\n",
    "def shuffle_and_split_data(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "\n",
    "train_set, test_set = shuffle_and_split_data(housing, 0.2)\n",
    "print(f\"Random split: train set = {len(train_set)}, test set = {len(test_set)}\")\n",
    "\n",
    "# --- Stable splitting using row index as ID ---\n",
    "from zlib import crc32\n",
    "\n",
    "def is_id_in_test_set(identifier, test_ratio):\n",
    "    return crc32(np.int64(identifier)) < test_ratio * 2**32\n",
    "\n",
    "def split_data_with_id_hash(data, test_ratio, id_column):\n",
    "    ids = data[id_column]\n",
    "    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))\n",
    "    return data.loc[~in_test_set], data.loc[in_test_set]\n",
    "\n",
    "housing_with_id = housing.reset_index()  # adds an 'index' column\n",
    "train_set_stable, test_set_stable = split_data_with_id_hash(housing_with_id, 0.2, \"index\")\n",
    "print(f\"Stable split: train set = {len(train_set_stable)}, test set = {len(test_set_stable)}\")\n",
    "\n",
    "# --- Stratified sampling by income category ---\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "    bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "    labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "\n",
    "# Check the proportions\n",
    "print(\"Income category proportions in test set:\")\n",
    "print(strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set))\n",
    "\n",
    "# Remove the income_cat column so data is back to original state\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e821fc5",
   "metadata": {},
   "source": [
    "### Why stratified sampling?\n",
    "\n",
    "- Stratified sampling ensures that the test set is representative of the overall dataset, especially for important attributes like income.\n",
    "- Random splits can lead to sampling bias, especially with small or imbalanced datasets.\n",
    "- After splitting, we drop the 'income_cat' column to revert the data to its original state.\n",
    "\n",
    "**Now we are ready to explore the training set!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd13700d",
   "metadata": {},
   "source": [
    "## 4. Explore and Visualize the Data to Gain Insights\n",
    "\n",
    "Now that we have a clean training set, let's explore it in depth. The goal is to understand the data, spot patterns, and discover relationships that will help us build better models.\n",
    "\n",
    "**Key steps:**\n",
    "- Visualize geographical data\n",
    "- Look for correlations\n",
    "- Experiment with attribute combinations\n",
    "\n",
    "Let's start by making a copy of the training set for exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31111eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the training set for exploration\n",
    "explore_set = strat_train_set.copy()  # This way, we can experiment freely\n",
    "\n",
    "# --- Geographical scatter plot ---\n",
    "import matplotlib.pyplot as plt\n",
    "explore_set.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True)\n",
    "plt.title(\"California Districts: Longitude vs Latitude\")\n",
    "plt.show()\n",
    "\n",
    "# --- Enhanced scatter plot with alpha for density ---\n",
    "explore_set.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2)\n",
    "plt.title(\"California Districts: Density Highlighted (alpha=0.2)\")\n",
    "plt.show()\n",
    "\n",
    "# --- Scatter plot with population and price color coding ---\n",
    "explore_set.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n",
    "                 s=explore_set[\"population\"] / 100, label=\"population\",\n",
    "                 c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n",
    "                 legend=True, sharex=False, figsize=(10, 7))\n",
    "plt.title(\"California Housing Prices: Color=Price, Size=Population\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2451608b",
   "metadata": {},
   "source": [
    "### Insights from Geographical Visualizations\n",
    "\n",
    "- The scatter plots show the geographical distribution of districts in California.\n",
    "- Using alpha (transparency) helps highlight high-density areas (e.g., Bay Area, Los Angeles, San Diego).\n",
    "- The color-coded plot reveals that housing prices are higher near the coast and in densely populated areas.\n",
    "- These visualizations suggest that location and population density are important predictors of housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a656cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Correlation analysis ---\n",
    "corr_matrix = explore_set.corr(numeric_only=True)\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)  # See which features are most correlated with price\n",
    "\n",
    "# --- Scatter matrix for selected attributes ---\n",
    "from pandas.plotting import scatter_matrix\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(explore_set[attributes], figsize=(12, 8))\n",
    "plt.suptitle(\"Scatter Matrix of Key Attributes\")\n",
    "plt.show()\n",
    "\n",
    "# --- Focused scatter plot: median_income vs median_house_value ---\n",
    "explore_set.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1, grid=True)\n",
    "plt.title(\"Median Income vs Median House Value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c65591",
   "metadata": {},
   "source": [
    "### Correlation and Attribute Insights\n",
    "\n",
    "- The correlation matrix shows that 'median_income' is the most strongly correlated feature with 'median_house_value'.\n",
    "- The scatter matrix helps visualize relationships between key attributes.\n",
    "- The focused scatter plot reveals a strong upward trend between median income and house value, but also shows capped values (horizontal lines), which may affect model performance.\n",
    "\n",
    "**Next, let's experiment with new attribute combinations to see if we can find even better predictors!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment with attribute combinations ---\n",
    "explore_set[\"rooms_per_house\"] = explore_set[\"total_rooms\"] / explore_set[\"households\"]\n",
    "explore_set[\"bedrooms_ratio\"] = explore_set[\"total_bedrooms\"] / explore_set[\"total_rooms\"]\n",
    "explore_set[\"people_per_house\"] = explore_set[\"population\"] / explore_set[\"households\"]\n",
    "\n",
    "# Check correlations again\n",
    "corr_matrix = explore_set.corr(numeric_only=True)\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d87d1e",
   "metadata": {},
   "source": [
    "### Attribute Engineering Insights\n",
    "\n",
    "- Creating new features like 'rooms_per_house', 'bedrooms_ratio', and 'people_per_house' can reveal stronger relationships with the target.\n",
    "- The 'bedrooms_ratio' attribute is more negatively correlated with house value than the raw bedroom or room counts.\n",
    "- 'rooms_per_house' is also more informative than 'total_rooms'.\n",
    "\n",
    "**Feature engineering is a powerful way to improve model performance!**\n",
    "\n",
    "Next, we'll prepare the data for machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e74eb0",
   "metadata": {},
   "source": [
    "## 5. Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "Now that we've explored the data and engineered some new features, it's time to prepare the data for machine learning algorithms.\n",
    "\n",
    "**Key steps:**\n",
    "- Handle missing values\n",
    "- Encode categorical attributes\n",
    "- Feature scaling\n",
    "- Build transformation pipelines for reproducibility and experimentation\n",
    "\n",
    "Let's start by separating the predictors and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ec0a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Separate predictors and labels ---\n",
    "housing = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n",
    "\n",
    "# --- Handle missing values with SimpleImputer ---\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "housing_num = housing.select_dtypes(include=[np.number])\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "# Show the learned medians\n",
    "print(\"Imputer statistics (medians):\", imputer.statistics_)\n",
    "print(\"Pandas medians:\", housing_num.median().values)\n",
    "\n",
    "# Transform the data (replace missing values with medians)\n",
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4de0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Encode categorical attribute ---\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "housing_cat = housing[[\"ocean_proximity\"]]\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "\n",
    "# Show the learned categories\n",
    "print(\"Categories:\", cat_encoder.categories_)\n",
    "\n",
    "# Convert the sparse matrix to a dense array (for display)\n",
    "housing_cat_1hot_array = housing_cat_1hot.toarray()\n",
    "housing_cat_1hot_array[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Feature scaling (Standardization) ---\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "housing_num_scaled = scaler.fit_transform(housing_tr)\n",
    "\n",
    "# Show the effect of scaling (mean ~0, std ~1)\n",
    "print(\"Means after scaling:\", housing_num_scaled.mean(axis=0))\n",
    "print(\"Stds after scaling:\", housing_num_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d350df49",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Machine learning algorithms do not perform well when the input numerical attributes have very different scales. For example, the `median_income` attribute ranges from 0 to 15, while the `total_rooms` attribute goes from 6 to 39,320. Feature scaling is essential to bring all attributes to the same scale.\n",
    "\n",
    "There are two common ways to get all attributes to have the same scale:\n",
    "- **Min-max scaling (normalization):** Values are shifted and rescaled so that they end up ranging from 0 to 1. This is done by subtracting the minimum value and dividing by the range (max - min).\n",
    "- **Standardization:** Values are centered around 0 with a standard deviation of 1. Standardization is less affected by outliers than min-max scaling.\n",
    "\n",
    "Here, we use `StandardScaler` from Scikit-Learn to standardize the features. This ensures that each attribute has a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917e53ea",
   "metadata": {},
   "source": [
    "### Transformation Pipelines\n",
    "\n",
    "As you can see, there are many data transformation steps that need to be executed in the right order. Scikit-Learn provides the `Pipeline` class to help with such sequences of transformations. Pipelines make the code cleaner and less error-prone, and they ensure that the same transformations are applied to both the training set and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e9f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# List of numerical and categorical attributes\n",
    "num_attribs = list(housing_num.columns)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "# Numerical pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"std_scaler\", StandardScaler()),\n",
    "])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "print(\"Transformed numerical data (first 5 rows):\\n\", housing_num_tr[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebff439b",
   "metadata": {},
   "source": [
    "The code above creates a pipeline for the numerical attributes. The pipeline first replaces missing values with the median of each attribute using `SimpleImputer`, then scales the features using `StandardScaler`. This ensures that all numerical features are on the same scale and that missing values are handled properly. Pipelines help keep the code clean and make it easy to apply the same transformations to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9e0937",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline: handle both numerical and categorical attributes\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, num_attribs),\n",
    "    (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "])\n",
    "\n",
    "housing_prepared = full_pipeline.fit_transform(housing_tr)\n",
    "print(\"Transformed data shape:\", housing_prepared.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2dbd08",
   "metadata": {},
   "source": [
    "The code above creates a full preprocessing pipeline using Scikit-Learn's `ColumnTransformer`. This pipeline applies the numerical pipeline to all numerical attributes and the `OneHotEncoder` to the categorical attribute. The result is a NumPy array containing all the processed features, ready to be fed into a machine learning algorithm. The shape of the transformed data shows the total number of features after all transformations, including the one-hot encoded categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d2bd09",
   "metadata": {},
   "source": [
    "## Select and Train a Model\n",
    "\n",
    "Now that the data is clean and ready for Machine Learning algorithms, the next step is to select and train a model. We will start by training a simple Linear Regression model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9368f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Train a Linear Regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Make predictions on the training set\n",
    "housing_predictions = lin_reg.predict(housing_prepared)\n",
    "lin_mse = mean_squared_error(housing_labels, housing_predictions)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "\n",
    "print(\"Linear Regression RMSE on training set:\", lin_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da59008",
   "metadata": {},
   "source": [
    "The code above trains a Linear Regression model using the prepared data. After fitting the model, it makes predictions on the training set and computes the Root Mean Squared Error (RMSE) to evaluate the model's performance. The RMSE gives an idea of how much error the model makes on average when predicting housing values. A lower RMSE indicates a better fit, but keep in mind that evaluating on the training set can be misleading if the model overfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be46803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Train a Decision Tree Regressor\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Make predictions on the training set\n",
    "housing_predictions_tree = tree_reg.predict(housing_prepared)\n",
    "tree_mse = mean_squared_error(housing_labels, housing_predictions_tree)\n",
    "tree_rmse = np.sqrt(tree_mse)\n",
    "\n",
    "print(\"Decision Tree RMSE on training set:\", tree_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dadba3",
   "metadata": {},
   "source": [
    "The code above trains a Decision Tree Regressor on the training data. After fitting the model, it makes predictions on the training set and computes the RMSE. Notice that the RMSE is much lower than with Linear Regression, which suggests that the model may be overfitting the training data. This is a common issue with powerful models like decision trees, which can fit the training data very closely but may not generalize well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5b463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluate Decision Tree with cross-validation\n",
    "tree_scores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-tree_scores)\n",
    "\n",
    "print(\"Decision Tree cross-validation RMSE scores:\", tree_rmse_scores)\n",
    "print(\"Mean:\", tree_rmse_scores.mean())\n",
    "print(\"Standard deviation:\", tree_rmse_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d330ab0",
   "metadata": {},
   "source": [
    "To get a better estimate of the model's performance, we use cross-validation. The `cross_val_score` function splits the training set into 10 folds, trains and evaluates the model 10 times, and returns the scores. The mean and standard deviation of the RMSE scores provide a more reliable estimate of the model's generalization performance. As you can see, the Decision Tree model performs much worse on the validation folds than on the training set, confirming that it is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd52442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Train a Random Forest Regressor\n",
    "forest_reg = RandomForestRegressor(random_state=42)\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n",
    "                               scoring=\"neg_mean_squared_error\", cv=10)\n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "\n",
    "print(\"Random Forest cross-validation RMSE scores:\", forest_rmse_scores)\n",
    "print(\"Mean:\", forest_rmse_scores.mean())\n",
    "print(\"Standard deviation:\", forest_rmse_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fa3c95",
   "metadata": {},
   "source": [
    "The code above trains a Random Forest Regressor, which is an ensemble of Decision Trees. It then evaluates the model using cross-validation. Random Forests generally perform better than individual Decision Trees by reducing overfitting and improving generalization. The mean and standard deviation of the RMSE scores provide an estimate of the model's performance and its stability across different folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f2676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid for Random Forest\n",
    "param_grid = [\n",
    "    {'n_estimators': [30, 50, 100], 'max_features': [6, 8, 10]},\n",
    "    {'bootstrap': [False], 'n_estimators': [30, 50], 'max_features': [6, 8, 10]},\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best estimator:\", grid_search.best_estimator_)\n",
    "\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1106d4a",
   "metadata": {},
   "source": [
    "The code above uses `GridSearchCV` to search for the best hyperparameters for the Random Forest model. It tries different combinations of `n_estimators` and `max_features`, both with and without bootstrapping, using 5-fold cross-validation. The best parameters and estimator are displayed, along with the RMSE for each combination. Grid search is a powerful way to systematically explore hyperparameter spaces and find the best model for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a54f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature importances from the best Random Forest model\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "\n",
    "# Get attribute names after one-hot encoding\n",
    "extra_attribs = [\"rooms_per_household\", \"population_per_household\", \"bedrooms_per_room\"]\n",
    "cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_one_hot_attribs = list(cat_encoder.categories_[0])\n",
    "attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n",
    "\n",
    "sorted_feat = sorted(zip(feature_importances, attributes), reverse=True)\n",
    "print(\"Feature importances:\")\n",
    "for score, name in sorted_feat:\n",
    "    print(f\"{name}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6785557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize feature importances\n",
    "feat_scores = sorted(zip(feature_importances, attributes), reverse=True)\n",
    "importances, names = zip(*feat_scores)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(names, importances)\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importances from Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e019ce6",
   "metadata": {},
   "source": [
    "The code above displays the feature importances computed by the best Random Forest model. Feature importances indicate how much each attribute contributes to the model's predictions. This information can help you understand the model and identify which features are most relevant for predicting house values. You can use this insight to select the most important features or to engineer new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184576bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model on the test set\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "# Prepare the test set\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "\n",
    "print(\"Final model RMSE on test set:\", final_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5757e6da",
   "metadata": {},
   "source": [
    "The code above evaluates the final model on the test set. The test set is transformed using the same preprocessing pipeline, and predictions are made using the best model found by grid search. The RMSE on the test set provides an unbiased estimate of the model's performance on new, unseen data. This is the final step in the end-to-end machine learning project, and it gives you a realistic idea of how well your model will perform in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff61b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "housing = pd.read_csv('housing.csv')\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37195ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()  # Quick description of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa0faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['ocean_proximity'].value_counts()  # Categorical attribute distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49589668",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()  # Summary of numerical attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7fc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed66eee1",
   "metadata": {},
   "source": [
    "## 3. Create a Test Set\n",
    "Let's split the data into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a02a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def shuffle_and_split_data(data, test_ratio):\n",
    "    shuffled_indices = np.random.permutation(len(data))\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test_indices = shuffled_indices[:test_set_size]\n",
    "    train_indices = shuffled_indices[test_set_size:]\n",
    "    return data.iloc[train_indices], data.iloc[test_indices]\n",
    "train_set, test_set = shuffle_and_split_data(housing, 0.2)\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac348999",
   "metadata": {},
   "source": [
    "## 4. Explore and Visualize the Data\n",
    "Let's explore the training set to gain insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee33fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = train_set.copy()\n",
    "housing.plot(kind='scatter', x='longitude', y='latitude', grid=True, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b141dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind='scatter', x='longitude', y='latitude', grid=True,\n",
    "             s=housing['population']/100, label='population',\n",
    "             c='median_house_value', cmap='jet', colorbar=True,\n",
    "             legend=True, sharex=False, figsize=(10, 7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51baf545",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix['median_house_value'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a8ea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "attributes = ['median_house_value', 'median_income', 'total_rooms', 'housing_median_age']\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind='scatter', x='median_income', y='median_house_value', alpha=0.1, grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a864dbe",
   "metadata": {},
   "source": [
    "## 5. Prepare the Data for Machine Learning Algorithms\n",
    "Let's handle missing values, encode categorical attributes, and scale features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb1ac95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "housing_num = housing.select_dtypes(include=[np.number])\n",
    "imputer.fit(housing_num)\n",
    "X = imputer.transform(housing_num)\n",
    "housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e074629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "housing_cat = housing[['ocean_proximity']]\n",
    "cat_encoder = OneHotEncoder()\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdda74a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "housing_num_scaled = scaler.fit_transform(housing_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb5720",
   "metadata": {},
   "source": [
    "## 6. Select and Train a Model\n",
    "Let's train a linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1795d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_num_scaled, housing['median_house_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b117b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "housing_predictions = lin_reg.predict(housing_num_scaled)\n",
    "lin_rmse = mean_squared_error(housing['median_house_value'], housing_predictions, squared=False)\n",
    "lin_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b21915c",
   "metadata": {},
   "source": [
    "## 7. Fine-Tune Your Model\n",
    "Try more complex models and use cross-validation for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc77d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "tree_reg = DecisionTreeRegressor(random_state=42)\n",
    "tree_reg.fit(housing_num_scaled, housing['median_house_value'])\n",
    "tree_predictions = tree_reg.predict(housing_num_scaled)\n",
    "tree_rmse = mean_squared_error(housing['median_house_value'], tree_predictions, squared=False)\n",
    "tree_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f54db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(tree_reg, housing_num_scaled, housing['median_house_value'],\n",
    "                         scoring='neg_root_mean_squared_error', cv=10)\n",
    "tree_rmse_scores = -scores\n",
    "tree_rmse_scores.mean(), tree_rmse_scores.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc05dd5",
   "metadata": {},
   "source": [
    "## 8. Present Your Solution and Next Steps\n",
    "Summarize findings, present model performance, and discuss deployment and monitoring."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
