# Chapter 2: End-to-End Machine Learning Project
## üéØ Easy Learning Guideline with Practical Python Code

**Your Goal:** Learn the complete ML project journey with hands-on code examples. Follow along, run the code, experiment, and learn!

---

## üìö What is This Chapter About?

Imagine you just joined a real estate company. Your boss says: **"Can you build a computer program that predicts house prices?"**

This chapter shows you **exactly how to do this** - from asking the right questions to launching your solution. The project uses real California house price data.

---

## üîß Setup: Install and Import Everything You Need

**Before you start, run this code first:**

```python
# Install libraries (run this once)
# !pip install pandas numpy matplotlib scikit-learn

# Import all the tools we'll use
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Set display options
pd.set_option('display.max_columns', None)
np.random.seed(42)  # For reproducibility

print("‚úÖ All imports successful! Ready to build an ML project!")
```

**What each library does:**
- **pandas**: Working with data (tables, spreadsheets)
- **numpy**: Math operations and arrays
- **matplotlib**: Creating charts and visualizations
- **scikit-learn**: Machine learning algorithms

---

## üîë The 8 Main Steps (Your Roadmap)

### **Step 1: Look at the Big Picture üî≠**

**What is this?** Understanding what you're trying to build before you start coding

**Key questions to ask:**
- "What problem are we solving?" (Will this help the company?)
- "How will the program be used?" (By a human? By another program?)
- "What counts as success?" (How good must it be?)

**Example from this project:**

```python
# Step 1: Frame the Problem
"""
PROBLEM DEFINITION:
- Goal: Predict median house prices in California districts
- Current Solution: Manual estimation by experts (takes time, 30% error)
- Business Use: Feed predictions to investment decision system
- Success Metric: Better than current 30% error estimate
- Problem Type: Supervised Learning + Regression (predict numbers, not categories)
"""

# Questions to answer:
print("=" * 50)
print("PROBLEM FRAMING CHECKLIST")
print("=" * 50)
print("1. Business Goal? ‚Üí Predict house prices for investment")
print("2. How will it be used? ‚Üí Downstream ML system will use it")
print("3. Current solution? ‚Üí Manual estimation by experts")
print("4. What success looks like? ‚Üí Better than 30% error")
print("5. Supervised or Unsupervised? ‚Üí SUPERVISED (we have labels)")
print("6. Classification or Regression? ‚Üí REGRESSION (predict numbers)")
print("7. Single or multiple outputs? ‚Üí UNIVARIATE (one price per house)")
print("8. Batch or Online learning? ‚Üí BATCH (data doesn't change rapidly)")
print("=" * 50)
```

---

### **Step 2: Get the Data üìä**

**What is this?** Downloading and loading your data

**Simple code to download and load:**

```python
# Step 2: Get the Data
from pathlib import Path
import tarfile
import urllib.request

def load_housing_data():
    """Download and load the California housing dataset"""
    # Define the path
    tarball_path = Path("datasets") / "housing.tgz"
    
    # Create directory if it doesn't exist
    if not tarball_path.is_file():
        Path("datasets").mkdir(parents=True, exist_ok=True)
        
        # Download the file
        url = "https://github.com/ageron/data/raw/main/housing.tgz"
        print(f"‚è≥ Downloading data from {url}...")
        urllib.request.urlretrieve(url, tarball_path)
        print("‚úÖ Download complete!")
        
        # Extract the file
        print("üì¶ Extracting data...")
        with tarfile.open(tarball_path) as housing_tarball:
            housing_tarball.extractall(path="datasets")
        print("‚úÖ Extraction complete!")
    
    # Load into pandas DataFrame
    csv_path = Path("datasets") / "housing" / "housing.csv"
    return pd.read_csv(csv_path)

# Load the data
housing = load_housing_data()
print(f"‚úÖ Data loaded! Shape: {housing.shape} (rows, columns)")
print(f"Memory usage: {housing.memory_usage().sum() / 1024**2:.2f} MB")
```

**Data info:**
- **Size:** 20,640 houses (rows) √ó 10 features (columns)
- **Format:** CSV (spreadsheet with commas)
- **Source:** California Census data from 1990

---

### **Step 3: Explore and Visualize the Data üëÄ**

**What is this?** Looking at the data carefully to understand patterns

#### **3A) Quick Look at the Data**

```python
# Step 3: Explore the Data
print("First 5 rows of data:")
print(housing.head())

print("\nData types and missing values:")
print(housing.info())

print("\nBasic statistics:")
print(housing.describe())

print("\nColumn names:")
print(housing.columns.tolist())
```

**Expected output:**
```
longitude     float64  20640 non-null
latitude      float64  20640 non-null
housing_median_age  float64  20640 non-null
total_rooms   float64  20640 non-null
total_bedrooms  float64  20433 non-null  ‚Üê 207 missing values!
population    float64  20640 non-null
households    float64  20640 non-null
median_income float64  20640 non-null
median_house_value  float64  20640 non-null  ‚Üê THIS IS OUR TARGET
ocean_proximity object   20640 non-null
```

#### **3B) Visualize Categorical Data**

```python
# Check the ocean_proximity column
print("Ocean Proximity values:")
print(housing['ocean_proximity'].value_counts())
```

**Output:**
```
1H OCEAN       9136
INLAND         6551
NEAR OCEAN     2658
NEAR BAY       2290
ISLAND            5
```

#### **3C) Create Histograms to See Distributions**

```python
# Create histograms for all numerical features
housing.hist(bins=50, figsize=(12, 8))
plt.suptitle("Distribution of Features", fontsize=16)
plt.tight_layout()
plt.show()

print("üìä What we learn from histograms:")
print("- Most house values cluster around $200-500K")
print("- Median income is scaled (not real dollars)")
print("- Population is right-skewed (long tail to the right)")
print("- Some features are capped (flat line at the top)")
```

#### **3D) Visualize Geographical Data**

```python
# Plot houses by location
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Left plot: Just location
housing.plot(kind='scatter', x='longitude', y='latitude', 
             alpha=0.1, ax=axes[0], title='House Locations')
axes[0].set_xlabel('Longitude')
axes[0].set_ylabel('Latitude')

# Right plot: Location + Price + Population
housing.plot(kind='scatter', x='longitude', y='latitude',
             s=housing['population']/100,  # Size = population
             c=housing['median_house_value'],  # Color = price
             cmap='jet', alpha=0.4, ax=axes[1],
             title='Location + Price (red=expensive) + Population (size)')
axes[1].set_xlabel('Longitude')
axes[1].set_ylabel('Latitude')
plt.colorbar(axes[1].collections[0], ax=axes[1], label='Median House Value')
plt.tight_layout()
plt.show()

print("üìç Geographic insights:")
print("- Bay Area (around -122, 37) = expensive! üî¥")
print("- Central Valley = cheaper üîµ")
print("- Coastal areas vary (north is cheaper than south)")
```

#### **3E) Find Correlations (What's Related?)**

```python
# Calculate correlation with target (median_house_value)
correlation_with_target = housing.corr()['median_house_value'].sort_values(ascending=False)

print("\nCorrelation with Median House Value:")
print("=" * 50)
for feature, corr in correlation_with_target.items():
    if feature != 'median_house_value':
        bar = "‚ñà" * int(abs(corr) * 30)
        print(f"{feature:20s} | {corr:7.3f} | {bar}")

print("\nüìä Interpretation:")
print("  +0.688 = median_income       (Strong! ‚Üëincome = ‚Üëprice)")
print("  +0.137 = total_rooms         (Weak positive)")
print("  -0.140 = latitude            (North is slightly cheaper)")
```

**Expected output:**
```
median_house_value   1.000000  
median_income        0.688380  ‚Üê STRONGEST!
total_rooms          0.137455  
housing_median_age   0.102175  
households           0.071426  
total_bedrooms       0.054635  
population          -0.020153  
longitude           -0.050859  
latitude            -0.139584  
```

#### **3F) Create Scatter Plot Matrix (See All Relationships)**

```python
from pandas.plotting import scatter_matrix

# Focus on promising features
attributes = ['median_house_value', 'median_income', 
              'total_rooms', 'housing_median_age']

scatter_matrix(housing[attributes], figsize=(12, 8))
plt.suptitle("Scatter Matrix of Key Features", fontsize=16)
plt.tight_layout()
plt.show()

print("üìà What to notice:")
print("- Diagonal shows histograms (distribution of each feature)")
print("- Other cells show scatter plots (relationships)")
print("- Median income ‚Üî house value shows strongest pattern")
```

#### **3G) Deep Dive: Income vs Price**

```python
# Create a detailed plot
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
housing.plot(kind='scatter', x='median_income', y='median_house_value',
             alpha=0.1)
plt.title('Median Income vs House Price')
plt.xlabel('Median Income (scaled, ~tens of thousands)')
plt.ylabel('Median House Value ($)')
plt.grid(True)

# Show the distribution
plt.subplot(1, 2, 2)
plt.hist(housing['median_house_value'], bins=50, edgecolor='black')
plt.title('Distribution of House Prices')
plt.xlabel('Median House Value ($)')
plt.ylabel('Frequency')
plt.axvline(x=500000, color='r', linestyle='--', label='Price Cap (500K)')
plt.legend()

plt.tight_layout()
plt.show()

print("üîç Important discovery:")
print("- STRONG correlation between income and price ‚úì")
print("- Price is capped at 500K (artificial limit) ‚ö†Ô∏è")
print("- Some prices at exactly 450K, 350K, 280K (rounded data?) ‚ö†Ô∏è")
```

---

### **Step 4: Prepare the Data for Algorithms üßπ**

**What is this?** Cleaning and transforming raw data

#### **4A) Handle Missing Values**

```python
# Step 4: Prepare the Data

print("Missing values in each column:")
print(housing.isnull().sum())
print(f"\nTotal rows: {len(housing)}")
print(f"Rows with missing total_bedrooms: 207 out of 20,640")

# Strategy 1: Delete rows with missing values (NOT RECOMMENDED)
housing_drop = housing.dropna(subset=['total_bedrooms'])
print(f"\nAfter deletion: {len(housing_drop)} rows (lost 207 rows) ‚ùå")

# Strategy 2: Fill missing values with median (RECOMMENDED)
median_bedrooms = housing['total_bedrooms'].median()
housing_filled = housing.copy()
housing_filled['total_bedrooms'].fillna(median_bedrooms, inplace=True)
print(f"\nAfter filling with median ({median_bedrooms}): {len(housing_filled)} rows ‚úÖ")

# Or use scikit-learn SimpleImputer (most professional way)
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='median')
housing_num = housing.select_dtypes(include=['number']).copy()
imputer.fit(housing_num)
housing_num_imputed = pd.DataFrame(
    imputer.transform(housing_num),
    columns=housing_num.columns
)

print(f"\nUsing SimpleImputer: {len(housing_num_imputed)} rows ‚úÖ")
print("Imputed statistics:")
print(f"  Median bedrooms: {imputer.statistics_[4]}")
```

#### **4B) Handle Categorical Data (Words ‚Üí Numbers)**

```python
# Categorical data: ocean_proximity
print("\nCategorical column: ocean_proximity")
print(housing['ocean_proximity'].unique())

# Method 1: Label Encoding (simple but bad for non-ordered categories)
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
housing['ocean_proximity_encoded'] = le.fit_transform(housing['ocean_proximity'])
print("\nLabel Encoding:")
for original, encoded in zip(le.classes_, le.transform(le.classes_)):
    print(f"  {original:15s} ‚Üí {encoded}")

# Method 2: One-Hot Encoding (better - creates binary columns)
from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder(sparse_output=False)
ocean_encoded = ohe.fit_transform(housing[['ocean_proximity']])
ocean_df = pd.DataFrame(
    ocean_encoded,
    columns=ohe.get_feature_names_out(['ocean_proximity'])
)
print("\nOne-Hot Encoding (first 5 rows):")
print(ocean_df.head())

print("\n‚úÖ One-Hot Encoding is better because:")
print("  - Treats each category equally (no false ordering)")
print("  - Works well with most algorithms")
```

#### **4C) Create New Features (Feature Engineering)**

```python
# Engineering new features from existing ones
housing_prepared = housing.copy()

# Feature 1: Rooms per household
housing_prepared['rooms_per_household'] = (
    housing['total_rooms'] / housing['households']
)

# Feature 2: Bedrooms ratio
housing_prepared['bedrooms_ratio'] = (
    housing['total_bedrooms'] / housing['total_rooms']
)

# Feature 3: People per household
housing_prepared['people_per_household'] = (
    housing['population'] / housing['households']
)

print("New engineered features:")
print(f"  rooms_per_household (avg rooms per household)")
print(f"  bedrooms_ratio (% of rooms that are bedrooms)")
print(f"  people_per_household (avg people per household)")

print("\nExample values:")
print(housing_prepared[['total_rooms', 'households', 'rooms_per_household']].head())

# Check if these correlate better with price
print("\nCorrelation with price (new features):")
print(f"  total_rooms:         {housing.corr()['median_house_value']['total_rooms']:.3f}")
print(f"  rooms_per_household: {housing_prepared.corr()['median_house_value']['rooms_per_household']:.3f}")
print("\n‚úÖ rooms_per_household has STRONGER correlation!")
```

#### **4D) Handle Skewed Distributions**

```python
# Some features are very skewed (stretched to one side)
# Solution: Apply log transformation

fig, axes = plt.subplots(2, 2, figsize=(12, 8))

features_to_transform = ['population', 'median_income']

for idx, feature in enumerate(features_to_transform):
    # Original distribution
    axes[0, idx].hist(housing[feature], bins=50, edgecolor='black')
    axes[0, idx].set_title(f'{feature} (original)')
    axes[0, idx].set_ylabel('Frequency')
    
    # Log-transformed distribution
    axes[1, idx].hist(np.log1p(housing[feature]), bins=50, edgecolor='black')
    axes[1, idx].set_title(f'{feature} (log-transformed)')
    axes[1, idx].set_ylabel('Frequency')

plt.tight_layout()
plt.show()

# Apply log transformation
housing_prepared['log_population'] = np.log1p(housing['population'])
housing_prepared['log_total_rooms'] = np.log1p(housing['total_rooms'])
housing_prepared['log_median_income'] = np.log1p(housing['median_income'])

print("‚úÖ Log transformation makes distributions more bell-shaped!")
```

#### **4E) Scale Features (Make Comparable)**

```python
# Problem: Features have different scales
# population: 0-35,000
# median_income: 0-15 (scaled)
# median_age: 1-52

print("Feature ranges (before scaling):")
print(f"  population:    {housing['population'].min():8.0f} - {housing['population'].max():8.0f}")
print(f"  median_income: {housing['median_income'].min():8.1f} - {housing['median_income'].max():8.1f}")
print(f"  housing_age:   {housing['housing_median_age'].min():8.0f} - {housing['housing_median_age'].max():8.0f}")

# Solution: Standardization (Z-score normalization)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
housing_num_cols = ['total_rooms', 'total_bedrooms', 'population', 
                    'households', 'median_income', 'housing_median_age']
housing_scaled = housing[housing_num_cols].copy()
housing_scaled_array = scaler.fit_transform(housing_scaled)

print("\nFeature ranges (after scaling):")
print(f"  mean = 0, std = 1 for all features")
print(f"  Most values between -3 and +3")

# Convert back to DataFrame to see it better
housing_scaled_df = pd.DataFrame(
    housing_scaled_array,
    columns=housing_num_cols
)
print("\nScaled data (first 5 rows):")
print(housing_scaled_df.head())

print("\nStatistics of scaled data:")
print(housing_scaled_df.describe())
```

---

### **Step 5: Split Data Into Training & Test Sets ‚úÇÔ∏è**

**What is this?** Dividing data into train (learn) and test (verify)

#### **5A) The Problem: Data Leakage**

```python
print("‚ö†Ô∏è  CRITICAL: Don't look at test data while building!")
print("Why? Your brain is a pattern detector that overfits!")
print("")
print("If you peek at test data, you might:")
print("  1. Choose the 'lucky' model that works for test data")
print("  2. Overestimate performance")
print("  3. Build a system that fails in production")
```

#### **5B) Simple Random Split (Not Recommended)**

```python
# Method 1: Simple random split ‚ùå (not ideal)
np.random.seed(42)

def shuffle_and_split_data(data, test_ratio):
    """Randomly split data (but not reproducible with new data)"""
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

train_set, test_set = shuffle_and_split_data(housing, 0.2)
print(f"Training set: {len(train_set)} rows")
print(f"Test set: {len(test_set)} rows")

print("\n‚ùå Problem: If you run this again with new data,")
print("   the test set will change (different rows)")
```

#### **5C) Better: Hash-Based Split (Reproducible)**

```python
# Method 2: Hash-based split ‚úÖ (reproducible)
from zlib import crc32

def is_id_in_test_set(identifier, test_ratio):
    """
    Use a hash to determine if an ID should go in test set.
    Same ID always goes to the same set, even if data changes.
    """
    return crc32(np.int64(identifier)) < test_ratio * 2**32

housing_with_id = housing.reset_index()  # Add index as ID

def split_data_with_id_hash(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]

train_set, test_set = split_data_with_id_hash(housing_with_id, 0.2, "index")
print(f"Training set: {len(train_set)} rows ‚úÖ (reproducible)")
print(f"Test set: {len(test_set)} rows ‚úÖ (reproducible)")
```

#### **5D) Best: Stratified Split (Representative)**

```python
# Method 3: Stratified split ‚úÖ‚úÖ (reproducible + representative)
# Ensures both train and test have same proportions of important features

# Create income categories
housing['income_cat'] = pd.cut(housing['median_income'],
                               bins=[0, 1.5, 3.0, 4.5, 6.0, np.inf],
                               labels=[1, 2, 3, 4, 5])

print("Income category distribution:")
print(housing['income_cat'].value_counts().sort_index())

# Use scikit-learn's stratified split
from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(
    housing,
    test_size=0.2,
    stratify=housing['income_cat'],  # Keep proportions
    random_state=42
)

print(f"\nTraining set: {len(train_set)} rows")
print(f"Test set: {len(test_set)} rows")

# Verify proportions are the same
print("\nIncome category proportions:")
print(f"Full dataset: {housing['income_cat'].value_counts(normalize=True).sort_index().values}")
print(f"Test set:    {test_set['income_cat'].value_counts(normalize=True).sort_index().values}")
print("\n‚úÖ Proportions match! Test set is representative!")

# Drop income_cat (we don't need it anymore)
train_set = train_set.drop('income_cat', axis=1)
test_set = test_set.drop('income_cat', axis=1)

print(f"\nFinal datasets:")
print(f"  Training: {train_set.shape}")
print(f"  Test:     {test_set.shape}")
```

---

### **Step 6: Select a Model and Train It ü§ñ**

**What is this?** Choosing an algorithm and letting it learn

#### **6A) Classify Your Problem**

```python
print("üîç Problem Classification:")
print("=" * 50)
print("1. Supervised or Unsupervised?")
print("   ‚Üí SUPERVISED (we have labels: actual prices) ‚úÖ")
print("")
print("2. Classification or Regression?")
print("   ‚Üí REGRESSION (predict numbers, not categories) ‚úÖ")
print("")
print("3. Univariate or Multivariate?")
print("   ‚Üí UNIVARIATE (predict one value: price) ‚úÖ")
print("")
print("4. Batch or Online Learning?")
print("   ‚Üí BATCH (data fits in memory, doesn't change rapidly) ‚úÖ")
print("")
print("5. Performance Metric?")
print("   ‚Üí RMSE (Root Mean Squared Error) ‚úÖ")
print("=" * 50)
```

#### **6B) Prepare Features and Labels**

```python
# Prepare training data
X_train = train_set.drop('median_house_value', axis=1)  # Features
y_train = train_set['median_house_value'].copy()        # Labels

X_test = test_set.drop('median_house_value', axis=1)
y_test = test_set['median_house_value'].copy()

print(f"Training features shape: {X_train.shape}")
print(f"Training labels shape:   {y_train.shape}")

# Handle missing values in test data
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='median')
X_train_imputed = pd.DataFrame(
    imputer.fit_transform(X_train.select_dtypes(include=['number'])),
    columns=X_train.select_dtypes(include=['number']).columns
)

X_test_imputed = pd.DataFrame(
    imputer.transform(X_test.select_dtypes(include=['number'])),
    columns=X_test.select_dtypes(include=['number']).columns
)

print("\n‚úÖ Missing values handled!")
```

#### **6C) Train a Simple Model: Linear Regression**

```python
# Train Linear Regression (simplest model to start)
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train_imputed, y_train)

print("‚úÖ Model trained!")
print(f"Model coefficients shape: {model.coef_.shape}")
print(f"Model intercept: {model.intercept_:,.0f}")

# Make predictions
y_train_pred = model.predict(X_train_imputed)
y_test_pred = model.predict(X_test_imputed)

print("\nFirst 5 predictions vs actual (training set):")
comparison = pd.DataFrame({
    'Actual': y_train.iloc[:5].values,
    'Predicted': y_train_pred[:5],
    'Error': y_train.iloc[:5].values - y_train_pred[:5]
})
print(comparison)
```

#### **6D) Evaluate Performance**

```python
# Calculate RMSE (Root Mean Squared Error)
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Training set performance
train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
train_mae = mean_absolute_error(y_train, y_train_pred)

# Test set performance
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
test_mae = mean_absolute_error(y_test, y_test_pred)

print("MODEL PERFORMANCE METRICS")
print("=" * 50)
print("Training Set:")
print(f"  RMSE: ${train_rmse:,.0f} (average prediction error)")
print(f"  MAE:  ${train_mae:,.0f}  (average absolute error)")
print("")
print("Test Set (what matters!):")
print(f"  RMSE: ${test_rmse:,.0f}")
print(f"  MAE:  ${test_mae:,.0f}")
print("=" * 50)

print("\nüìä Interpretation:")
print(f"  - On average, predictions are off by ~${test_rmse:,.0f}")
print(f"  - That's about {test_rmse/y_test.mean()*100:.1f}% of average price")
print(f"  - Compare to current 30% error = SUCCESS! ‚úÖ")

# Visualize predictions vs actual
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.scatter(y_test, y_test_pred, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Predictions vs Actual (Test Set)')
plt.grid(True)

plt.subplot(1, 2, 2)
errors = y_test - y_test_pred
plt.hist(errors, bins=50, edgecolor='black')
plt.xlabel('Prediction Error ($)')
plt.ylabel('Frequency')
plt.title('Distribution of Errors')
plt.axvline(x=0, color='r', linestyle='--')
plt.grid(True)

plt.tight_layout()
plt.show()
```

---

### **Step 7: Fine-Tune Your Model üéöÔ∏è**

**What is this?** Improving performance by adjusting settings

#### **7A) Cross-Validation (Better Evaluation)**

```python
# Simple approach: one train-test split might be lucky/unlucky
# Better approach: cross-validation (test on multiple splits)

from sklearn.model_selection import cross_val_score

# Use 5-fold cross-validation
cv_scores = cross_val_score(
    model,
    X_train_imputed,
    y_train,
    cv=5,  # 5 splits
    scoring='neg_mean_squared_error'
)

# Convert back to RMSE
cv_rmse_scores = np.sqrt(-cv_scores)

print("Cross-Validation Results (5-fold)")
print("=" * 50)
for fold, score in enumerate(cv_rmse_scores, 1):
    print(f"Fold {fold}: ${score:,.0f}")
print("-" * 50)
print(f"Mean RMSE: ${cv_rmse_scores.mean():,.0f}")
print(f"Std Dev:   ${cv_rmse_scores.std():,.0f}")
print("=" * 50)

print("\n‚úÖ Cross-validation gives a more reliable estimate!")
```

#### **7B) Try Different Models**

```python
# Train multiple models and compare
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

models = {
    'Linear Regression': LinearRegression(),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(n_estimators=10, random_state=42)
}

results = {}

for name, model in models.items():
    # Train
    model.fit(X_train_imputed, y_train)
    
    # Evaluate on test set
    y_pred = model.predict(X_test_imputed)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    mae = mean_absolute_error(y_test, y_pred)
    
    # Cross-validation score
    cv_scores = cross_val_score(model, X_train_imputed, y_train, cv=5,
                                scoring='neg_mean_squared_error')
    cv_rmse = np.sqrt(-cv_scores.mean())
    
    results[name] = {'RMSE': rmse, 'MAE': mae, 'CV_RMSE': cv_rmse}
    
    print(f"\n{name}:")
    print(f"  Test RMSE: ${rmse:,.0f}")
    print(f"  CV RMSE:   ${cv_rmse:,.0f}")

# Compare
print("\n" + "=" * 50)
print("MODEL COMPARISON")
print("=" * 50)
comparison_df = pd.DataFrame(results).T
comparison_df = comparison_df.sort_values('CV_RMSE')
print(comparison_df)
```

#### **7C) Hyperparameter Tuning with Grid Search**

```python
# Random Forest has many hyperparameters (knobs to adjust)
# Use GridSearchCV to find the best combination

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [10, 30, 100],      # Number of trees
    'max_depth': [5, 10, None],         # Tree depth
    'min_samples_split': [2, 5, 10]     # Samples to split node
}

print("üîß Grid Search: Testing all combinations...")
print(f"Total combinations to test: {3 * 3 * 3} = 27")

grid_search = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid,
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1  # Use all processors
)

grid_search.fit(X_train_imputed, y_train)

print("\n‚úÖ Grid Search Complete!")
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best CV RMSE: ${np.sqrt(-grid_search.best_score_):,.0f}")

# Evaluate best model on test set
best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test_imputed)
test_rmse_best = np.sqrt(mean_squared_error(y_test, y_pred_best))

print(f"Test RMSE: ${test_rmse_best:,.0f}")
```

#### **7D) Feature Importance (Which Features Matter?)**

```python
# Random Forest can tell us which features are most important
feature_importance = best_model.feature_importances_
feature_names = X_train_imputed.columns

# Sort by importance
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importance
}).sort_values('Importance', ascending=False)

print("\nFeature Importance:")
print("=" * 50)
for _, row in importance_df.iterrows():
    bar = "‚ñà" * int(row['Importance'] * 100)
    print(f"{row['Feature']:25s} {row['Importance']:6.3f} {bar}")

# Visualize
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Importance')
plt.title('Feature Importance (Random Forest)')
plt.tight_layout()
plt.show()
```

---

### **Step 8: Launch, Monitor, and Maintain üöÄ**

**What is this?** Deploying and maintaining your model

#### **8A) Save Your Model**

```python
# Save the trained model for later use
import joblib

# Save the model
joblib.dump(best_model, 'california_housing_model.pkl')
print("‚úÖ Model saved to 'california_housing_model.pkl'")

# Later, load it like this:
loaded_model = joblib.load('california_housing_model.pkl')
print("‚úÖ Model loaded successfully!")

# Test that it works
test_pred = loaded_model.predict(X_test_imputed[:5])
print("\nTest predictions:")
for i, pred in enumerate(test_pred):
    print(f"  House {i+1}: ${pred:,.0f}")
```

#### **8B) Make Predictions on New Data**

```python
# Simulate new districts coming in
new_data = X_test_imputed.iloc[:3].copy()

print("Making predictions on new data:")
print("=" * 50)

predictions = best_model.predict(new_data)

for i, (idx, row) in enumerate(new_data.iterrows()):
    print(f"\nNew District {i+1}:")
    print(f"  Prediction: ${predictions[i]:,.0f}")
    print(f"  Actual:     ${y_test.iloc[idx]:,.0f} (if known)")
```

#### **8C) Create a Pipeline for Automation**

```python
# Pipeline: automatically apply all transformations in sequence
from sklearn.pipeline import Pipeline as SklearnPipeline

# Create pipeline
full_pipeline = SklearnPipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler()),
    ('model', RandomForestRegressor(n_estimators=30, random_state=42))
])

# Train on original data (with missing values, unscaled)
full_pipeline.fit(X_train, y_train)

# Now you can predict directly on raw data!
y_pred_pipeline = full_pipeline.predict(X_test)
rmse_pipeline = np.sqrt(mean_squared_error(y_test, y_pred_pipeline))

print("Pipeline RMSE:", f"${rmse_pipeline:,.0f}")
print("\n‚úÖ Pipeline automates everything:")
print("  1. Handle missing values")
print("  2. Scale features")
print("  3. Make predictions")
print("  All in one go!")
```

#### **8D) Monitor Performance Over Time**

```python
# In production, you need to monitor if the model still works

print("üìä MONITORING CHECKLIST")
print("=" * 50)
print("After launching, check regularly:")
print("")
print("1. Performance Metrics")
print("   - Is RMSE still low?")
print("   - Are predictions within expected range?")
print("")
print("2. Data Drift")
print("   - Are new house prices changing?")
print("   - Are income distributions shifting?")
print("")
print("3. Error Analysis")
print("   - Which districts get wrong predictions?")
print("   - Are there patterns to the errors?")
print("")
print("4. Retraining Schedule")
print("   - Retrain monthly with new data")
print("   - Retrain if performance drops >5%")
print("=" * 50)

# Example: monthly performance check
def monitor_model_performance(y_true, y_pred, threshold_rmse=75000):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    
    print(f"\nüìà Monthly Report:")
    print(f"  RMSE: ${rmse:,.0f} ", end="")
    if rmse < threshold_rmse:
        print("‚úÖ (Good)")
    else:
        print("‚ö†Ô∏è  (Needs retraining)")
    
    print(f"  MAE:  ${mae:,.0f}")
    
    return rmse < threshold_rmse

is_healthy = monitor_model_performance(y_test, y_pred_best)
```

---

## üìñ Key Concepts Summary

### **Understanding Notation**

```python
# Single training example:
x¬π = [118.29, 33.91, 1416, 38372]  # Features of house 1
y¬π = 156,400                         # Price of house 1

# All training data:
X = matrix of shape (16512, 9)      # All features
y = vector of shape (16512,)        # All prices

# Model's prediction:
≈∑¬π = h(x¬π) = model.predict([x¬π])   # Predicted price for house 1
```

### **Performance Metrics Explained**

```python
# RMSE: Average error (big mistakes hurt more)
# Formula: sqrt( (1/m) * sum of (predicted - actual)¬≤ )
# Use when: outliers are rare (normal distribution)

# MAE: Average absolute error (all mistakes equal)
# Formula: (1/m) * sum of |predicted - actual|
# Use when: outliers are common

# Example:
# If RMSE = $60,000:
#   "On average, predictions are off by about $60,000"
```

### **Why Different Scales Matter**

```python
# Without scaling:
# population:    0 to 35,000 (huge numbers)
# median_income: 0 to 15     (tiny numbers)

# Algorithm might think population is 1000x more important!
# Solution: Standardize all features to mean=0, std=1
```

---

## üéØ Complete Workflow in One Script

```python
# Complete ML project from start to finish

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, mean_absolute_error
import joblib

# 1. Load data
housing = pd.read_csv('datasets/housing/housing.csv')

# 2. Explore (you would do more visualization here)
print(f"Data shape: {housing.shape}")
print(f"Correlations with price:\n{housing.corr()['median_house_value']}")

# 3. Create train/test split (before exploring further!)
train_set, test_set = train_test_split(
    housing, test_size=0.2, random_state=42
)

# 4. Separate features and labels
X_train = train_set.drop('median_house_value', axis=1)
y_train = train_set['median_house_value']
X_test = test_set.drop('median_house_value', axis=1)
y_test = test_set['median_house_value']

# 5. Prepare data
X_train_num = X_train.select_dtypes(include=['number'])
X_test_num = X_test.select_dtypes(include=['number'])

# 6. Create and train pipeline
pipeline = Pipeline([
    ('impute', SimpleImputer(strategy='median')),
    ('scale', StandardScaler()),
    ('model', RandomForestRegressor(n_estimators=30, random_state=42))
])

pipeline.fit(X_train_num, y_train)

# 7. Evaluate
y_pred = pipeline.predict(X_test_num)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print(f"\nFinal RMSE: ${rmse:,.0f}")

# 8. Save
joblib.dump(pipeline, 'model.pkl')
print("‚úÖ Model saved!")
```

---

## üß† Learning Path

**Week 1: Understand the Process**
- [ ] Read this guide 1-2 times
- [ ] Run all code examples
- [ ] Change hyperparameters and see what happens

**Week 2: Read the Book**
- [ ] Read Chapter 2 carefully
- [ ] Code along with the book examples
- [ ] Compare the book code with this guide

**Week 3: Apply to New Dataset**
- [ ] Find a different dataset (Kaggle, UCI ML Repository)
- [ ] Apply all 8 steps
- [ ] Document your process

**Week 4: Deep Dive**
- [ ] Understand the math behind algorithms
- [ ] Try more complex models
- [ ] Learn about hyperparameter tuning

---

## ‚úÖ Checklist for Your Project

Before launching any ML system:

- [ ] Step 1: Did you clearly define the problem?
- [ ] Step 2: Did you get all the data you need?
- [ ] Step 3: Did you explore patterns thoroughly?
- [ ] Step 4: Did you handle missing values and scaling?
- [ ] Step 5: Did you create train/test sets FIRST?
- [ ] Step 6: Did you try multiple models?
- [ ] Step 7: Did you fine-tune the best model?
- [ ] Step 8: Did you save the model and document it?

---

## üéì Next Steps

1. **Run the code:** Copy-paste code examples into Jupyter
2. **Modify:** Change parameters and see what happens
3. **Explore:** Try different models and preprocessing steps
4. **Read:** Read Chapter 2 of the book for deeper understanding
5. **Build:** Find a new dataset and repeat all 8 steps

---

## üí° Final Wisdom

> "Machine Learning is 80% data preparation, 20% algorithms. Master the process, not just the formulas."

The 8 steps in this chapter work for **ANY** ML problem:
- Predicting house prices ‚úì
- Classifying emails as spam ‚úì
- Recommending movies ‚úì
- Detecting diseases ‚úì
- Forecasting stock prices ‚úì

Once you understand the process, you can apply it everywhere!

**Happy learning! üöÄ**

